Towards Understanding Best Practices for
Quantization of Vision-Language Models
Gautom Das∗ Vincent La∗ EthanLau∗ AbhinavShrivastava MatthewGwilliam
University of Maryland, College Park
arXiv:2601.15287v1  [cs.CV]  21 Jan 2026
Abstract
Large language models (LLMs) deliver impressive results for a variety of tasks,
but state-of-the-art systems require fast GPUs with large amounts of memory.
To reduce both the memory and latency of these systems, practitioners quantize
their learned parameters, typically at half precision. A growing body of research
focuses on preserving the model performance with more aggressive bit widths,
and some work has been done to apply these strategies to other models, like
vision transformers. In our study we investigate how a variety of quantization
methods, including state-of-the-art GPTQ and AWQ, can be applied effectively
to multimodal pipelines comprised of vision models, language models, and their
connectors. We address how performance on captioning, retrieval, and question
answering can be affected by bit width, quantization method, and which portion of
the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit
comparable importance in model performance, despite significant differences in
parameter size, and that lower-bit quantization of the LLM achieves high accuracy
at reduced bits per weight (bpw). These findings provide practical insights for
efficient deployment of MLLMs and highlight the value of exploration for under
standing component sensitivities in multimodal models. Our code is available at
https://github.com/gautomdas/mmq.
1 Introduction
As neural networks grow larger, especially with the rise of large language models (LLMs) [1], it has
become increasingly important to reduce memory and computational demands [2]. Training these
models demands massive GPU/TPU clusters, and even inference can be costly [3]. Additionally,
distributed multi-GPU setups are not practical for model deployment on edge devices and in similar
real-world scenarios [4]. As these models become more ubiquitous, their cost and accessibility
become massively important societal issues.
While text-only LLMs are expensive enough on their own, real-world applications increasingly
require models that can process not only text, but also audio and visual data, leading to the rise of
multimodal LLMs (MLLMs) [5, 6]. Multimodal large language models (MLLMs), which combine
LLMs with vision encoders (and/or audio encoders [7]), require even more resources, and exacerbate
cost and accessibility issues. While these can deliver impressive performance on multimodal retrieval,
captioning, and question-answering tasks, the performance typically scales with model size and
latency. Video data drives additional massive increases in latency and memory consumption.
Model compression, particularly quantization, offers a potential solution to the growing model size
problem [2]. One goal of model compression is to find where “free lunch” ends– how small we
can make a model without sacrificing performance. Another goal is to aggressively compress the
model beyond this point, while mitigating the penalty to performance. Quantization tackles this by
*Equal contribution.
Preprint. Under review.
4 6 8 10 12 14 16
Bits per Weight
0.0
0.2
0.4
0.6
0.8
1.0
1.2
BLIP-2, COCO Captions, CIDEr
Quantization Method
GPTQ AWQ
3 4 5 6 7 8 9
Bits per Weight
54
55
56
57
58
59
60
61
62
LLaVA w/ GPTQ, GQA Accuracy
GPTQ Quantization Bits (ViT and LLM)
3-bit ViT 16-bit ViT 3-bit LLM 8-bit LLM
Figure1:Highlightedanalysis. Inthiswork,weinvestigatehowquantizationsaffectsmultimodal
modelsonavarietyoftasks,specificallyBLIP-2(left)andLLaVA(right).Wecomparestate-of-the
artquantizationstrategies(left)andalsodiscusswhichportionsofthepipelinearemostamenableto
quantization(right).
preservingallmodelweightsandtheirrelativeapproximatevalues,butchangingtheprecisionat
whichthesevaluesarestored.
Onemightassumethatifweuniformlyquantizeweights,modelperformancewilldecreasewithsome
linearcorrelationtothereductioninbitwidth. Inreality,asweshowinFigure1,performancevaries
dramaticallydependingonwhichpartsofanMLLMarequantized,andwhichmethodischosenfor
thequantization.Decisionslikewhichmethodtouse(aswecomparestate-of-the-artsGPTQ[8]and
AWQ[9])candramaticallyaffectperformanceatlowerbitwidths.QuantizingtheViTcomparedto
theLLMhasdramaticallydifferenteffectsontheperformance-sizetrade-off,consideringthatwhile
bothaffecttheperformance,quantizingtheLLMhasamuchhigherimpactonthesizeoftheoverall
MLLM.Understandingmultimodalquantizationiskeytoimprovingmodelefficiency,asitreveals
whichcomponentsaremostcriticalforperformance.Notallcomponentsareequallysensitivetoa
reductioninprecision,andthussomearemoretolerabletoquantizationtechniques.Byminimizing
informationlossacrosssalientmodelcomponents,wecandeployquantizedmultimodalmodelsthat
optimizethemodelsize/taskperformancetrade-off.
Insummary,wedistillseveralkeyprinciplesgoverningMLLMsandquantizationstrategies.First,
componentsensitivityvariessubstantiallyacrossmultimodalarchitectures,withlanguagemodels
generallyrequiringhigherprecisionthanvisioncomponentsregardlessoftheirparametercount.
Second,state-of-the-artmethodslikeGPTQandAWQeffectivelypreservemodelperformanceat
significantlylowerbitwidths(3.5-4.5bpw)comparedtouniformapproaches.Third,taskcharacteris
ticsfundamentallydetermineoptimalbitallocations—reasoningtasksheavilyfavorLLMprecision
whilevisual-textualalignmenttasksshowmorebalancedrequirements.Fourth,thechoiceofquanti
zationmethoddramaticallyredistributescomponentimportance,withAWQconcentratingonLLM
preservationwhileGPTQdistributesimportancemoreevenly.Finally,architecturaldependencies
createinteractioneffectsthatnecessitateholisticpipelineanalysisratherthanindependentcomponent
evaluation. TheseprinciplesprovidepracticalguidanceforefficientMLLMdeploymentacross
diversetasksandcomputationalconstraints.
2 RelatedWork
2.1 VisionLanguageModels
Visionlanguagemodels(VLMs)areasubsetofmultimodalmodelswhoseinputsandoutputscom
priseimages,text,andsometimesvideos.Theseapproachesaretypicallytrainedusingvideo/image
textpairstoalignvisionandlanguageinputsinasharedspace[10–14]. Inthiswork,wemakea
distinctionbetweentheseVLMs,whichtypicallyfocusongeneratingembeddingsofimagesandtext
fortaskslikeretrieval,andvisionlargelanguagemodels(VLLMs),whichtypicallyconsumeimage
andtextinputstogeneratelanguageoutputs[5,15–20],suchasanswerstocomplicatedquestions
aboutspecificimages[21,22].BLIP-2[5]alleviatestheexpensivepotentialtrainingcostofVLLMs
2
Text Output
LLM
Connector
Text
Vision
Encoder
Image
(a) Model Components
Feed
Forward
Front
Middle
End
(b) Block Groups
Attention
(c) Layer Types
Algorithm 1 Uniform Quantization Experi
ments
1: for k ∈ {2,4,6,8} do
2:
for all C ⊆ {ViT,LLM,QFormer} do
3:
4:
5:
for all B ⊆ {front,middle,end} do
for all M ⊆ {attn,FF} do
quantize(k, C,B,M)
Figure 2: We investigate the sensitivity of the various parts of an MLLM to quantization, specifically
in terms of model components, block groups, and layer types. For uniform quantization, we perform
a dense grid search of bit widths and combinations of these components. For state-of-the-art methods
(GPTQ and AWQ)weperform a more targeted analysis.
by introducing a Q-Former to bridge the gap between frozen image models [11, 23] and frozen
language models [24]. LLaVA [6] uses a simpler projector to connect the frozen models, but finetunes
both using instruction tuning. In this work, we investigate how quantization affects both BLIP-2 and
LLaVA as representative VLLMs.
2.2 Model Compression
Pruning seeks to remove neurons of low saliency, often resulting in sparser computation. This can be
done in an unstructured manner [25–28], where individual weights are removed, wherever they may
occur, or in a structured manner [29–31], where entire layers are removed. While this can greatly
reduce model size and FLOPs, it often comes at the cost of model accuracy. The pruning process can
be done iteratively to monitor performance degradation, or fine-tuning can be done afterward to help
recover the performance of the full-sized model.
Knowledge distillation (KD) involves training a smaller, compact student model to mimic the
behavior of a larger teacher model. The student model uses logits from the larger teacher as soft
targets during training [32–37]. KD often struggles to achieve high compression ratios on its own,
when compared to pruning or quantization, as an overly compressed student model can struggle to
approximate the complexity of the teacher model.
Quantization refers to a reduction in the numerical precision of neural network weights and acti
vations [38, 39]. It is often utilized for the deployment of models on edge devices, achieving more
efficient inference by leveraging low-bit integer arithmetic [2]. Quantization methods tend to fall
into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ
involves quantizing weights/activations after training a full-precision model, often using a representa
tive calibration set to determine scales, clipping ranges, and the saliency of parameters [8, 9, 40–47].
QAT involves quantizing a model and then training/fine-tuning a model to adjust parameters to
recover model performance degradation [48–52]. PTQ is more lightweight than QAT as it does
not require additional training. Recent interest in extreme quantization of LLMs has even resulted
in a third paradigm, related to QAT, where the architecture itself is designed with quantization in
mind, utilizing ternary and even binary weights [53–55]. Quantization can be combined with pruning
or distillation of knowledge for even larger amounts of model compression [56–58]. In this work,
we focus specifically on quantization methods which may require calibration, but do not require
any additional training/fine-tuning. These include uniform quantization, as well as state-of-the-art
(SOTA) methods, GPTQ [8] and AWQ [9]. Instead of applying these to unimodal models (LLMs),
we investigate how well they work in the multimodal setting.
3 Optimizing Performance
3.1 Building Intuitions
For our preliminary experiment, we explore the impact of reduced bit widths on different parts
of the BLIP-2 architecture via simulated uniform quantization. At a high level, this procedure
3
8 10 12 14 16
Bits per Weight
1.24
1.25
1.26
1.27
1.28
1.29
BLIP-2, COCO, CIDEr
Quantized Model
ViT
V+Q
 
Q-Former
V+L
All
LLM
Q+L
8 10 12 14 16
Bits per Weight
1.24
1.25
1.26
1.27
1.28
1.29
Quantized Layer
Attn FF Both
8 10 12 14 16
Bits per Weight
1.24
1.25
1.26
1.27
1.28
1.29
Quantized Group
Front
F+M
 
Middle
F+E
All
End
M+E
Figure3:UniformquantizationimpactonCOCOcaptioningforBLIP-2.Wefocustheseplots
onthehigh-qualityrange(seeappendixforfull results),drawingattentionfirst towhichmodel
componentwequantize(left), thenlayer type(middle), thengroup(right).Weprovide2black
starsoneachplot,indicatingtheresultsforquantizingtheentirepipelineat8and16bits.Thebest
performance-sizetradeoffstendtocomewhenquantizingtheentirepipeline.
involvesnormalizingtheweights,discretizingthem,andthenreturningthemtotheiroriginalscalefor
inference.Forthederivationofthek-bituniformquantizationformulaused,pleaseseetheAppendix.
Afterselectingabitwidth,k,from{2,4,6,8},wesubdividetheBLIP-2architecturealongdifferent
axesshowninFigure2,andweinvestigatetheimpactofk-bituniformquantization.Firstweconsider
“modelcomponents”,whicharetheViTandtheQ-Formerforretrieval,andtheViT,LLM,and
Q-Formerforcaptioning.Wealsocandividetheseintermsof“blockgroups,”wherewespliteach
modelcomponentinto3equal-size,contiguousgroupsofblocks,whichwecallfront,middle,and
end.Finally,wetargetthequantizationbydifferent“layertypes,”applyingittovaryingcombinations
ofattention(attn)andfeedforward(FF)layers.
WeshowresultsforthisexplorationinFigure3.Overall,wedonotobserveasignificantcorrelation
betweensizeandperformanceforlayertypesorblockgroups.Simplyquantizingbothlayertypes
andallblockgroupsisnecessaryforbestresults.Also,whilesimplyquantizingtheentirepipelineat
8bitsisapointalongthePareto-frontier, therearestillmanypointsalongthefrontierthatdonot
quantizeeverything,especiallyintermsofmodelcomponents.Thesefindingsarelargelyconsistent
ontheLLM-freeretrievaltaskaswell,asweshowintheappendix.Therefore,wefocusthemajority
ofouranalysisinthissectiononthemodelcomponents(ViT,Q-Former,LLM,orequivalents).
3.2 State-of-the-ArtQuantizationPreliminaries
Followingourpreliminaryquantizationexperiments,weapplystate-of-the-art(SOTA)unimodal
quantizationmethods,Activation-awareWeightQuantization(AWQ)andGPTQtoBLIP-2and
LLaVA.
AWQ[9]isaweight-onlyPTQtechniqueforLLMsthatidentifiessalientweightchannelsbyreferring
totheactivationdistributionofasmall, representativecalibrationset.Weightsthatyieldgreater
activationmagnitudescorrespondtomorecriticalfeaturesandaredeemedtobemoresalient.AWQ
reducesquantizationerrorbypreservingjust1%ofsalientweightsviaaper-channelscalingfactor.
GPTQ[8]isanotherweight-onlyPTQtechniqueforLLMsbutleveragesapproximatesecond-order
informationderivedfromtheinverseHessianinsteadofactivationinformation. Asweightsare
processedandquantizedsequentially,theremainingunquantizedweightsareadjustedtocompensate
forquantizationerror,computedwithasmallcalibrationset.GPTQreplacesiterativeHessianupdates
withaCholesky-basedreformulationforgreaternumericalstabilitywhenhandlingbillion-parameter
LLMs.
4
Figure 4: SOTA Unimodal Quantization Performance-Size Tradeoff for retrieval, captioning, and
VQAtasks. WeapplyGPTQandAWQtoentiremodelcomponents(visionencoder, connector, LLM),
of BLIP-2 and LLaVA. Unimodal SOTA methods are able to preserve full-precision performance at
high compression rates, even in the multimodal context.
Since we observe in Section 3.1 that the choice of block group has little to no impact on quan
tized model performance, we conduct these experiments with a coarser granularity. We quantize
entire model components (vision encoder, LLM, Q-Former) to k-bits. In addition, we expand our
benchmarks to include visual question-answering (VQA) on VQAv2 [22] and GQA [21]. Both are
successors of prior VQA datasets that contained biases, giving a false sense of understanding.
3.3 State-of-the-art Quantization Benchmark
For each task, we randomly sample a set of 128 image and text pairs from the respective dataset to
serve as the calibration set for AWQ and GPTQ. We evaluate BLIP-2 ViT-g for retrieval, and BLIP-2
ViT-g OPT 2.7B for captioning and VQA tasks. Additionally, we evaluate LLaVA 1.5 7B for VQA
tasks. We evaluate on 10% of the val2014 split for VQAv2, which equates to 21435 samples, and on
the entire Test-Dev split for GQA.
It is important to note that typically, when unimodal SOTA quantization methods are applied to
MLLMs, only the language model component is quantized. In our search space, we include configu
rations where we also quantize the vision model and connector components. Our SOTA quantization
experiment search space selects bit widths from {2,3,4,5,6,8} and considers model components
C ⊆{ViT,LLM,Q-Former} for BLIP-2 and model components C ⊆ {ViT,LLM} for LLaVA.
Weshowresults for our SOTA experiments in Figure 4. Overall, the unimodal SOTA methods are able
to preserve task performance better at more extreme bit widths than uniform quantization. For uniform
quantization, the configurations with the lowest bpw that achieved comparable task performance to
the full-precision model fell around 6.0-8.0 for retrieval tasks and 8.0-10.0 for captioning. For the
SOTA methods, this optimal performance-size trade-off falls into the 3.5-4.5 bpw range for all tasks.
Weobserve that AWQ tends to degrade in model performance more steeply in the extreme bit-width
regime than GPTQ for captioning and VQA tasks. This trend can be viewed across both the BLIP-2
and LLaVA architectures. The opposite is true, however, for retrieval where AWQ configurations
outperform GPTQ configurations at lower bpw.
5
50
AWQ, VQAv2 Accuracy
40
30
20
10
0
50
2
4
6
Bit Width
8
40
30
20
10
GPTQ, VQAv2 Accuracy
0
2
6
4
Bit Width
8
30
20
10
AWQ, GQA Accuracy
0
30
20
10
GPTQ, GQA Accuracy
0
2
6
8
2
6
8
4
Bit Width
Quantized Component
ViT Q-Former LLM
4
Bit Width
Figure 5: Individual quantization of BLIP-2’s ViT, Q-Former, and LLM for VQAv2 and GQA. We
show that the different components have varying sensitivities in regards to bit width and quantization
method. The LLM tends to have the highest sensitivity in question-answering tasks, followed by the
ViT and Q-Former.
3.4 Component Impact Ablations
From our SOTAquantization experiments, we pulled configurations in which only a single component
is quantized for Figure 5 and configurations in which two components are quantized for Figure 6.
Figure 5 gives an estimate of how each component individually affects the model’s performance in
VQA. Wenotice that for both AWQ and GPTQ, the LLM tends to be the most sensitive and drops
off earlier than the ViT and the Q-Former. The Q-Former has the least sensitivity, but it also has the
fewest parameters. Surprisingly, the ViT experiences a large drop-off with GPTQ, but it experiences
relatively little degradation with AWQ.
Figure 6 shows the interaction between components when quantized. Empirically, the ViT and LLM
tend to have a stronger influence on other components, which could be due to their large parameter
sizes. Notably, the LLM has the strongest influence on performance, as seen by the complete loss of
performance at around 7 bpw when quantizing the LLM at a lower bit width than the ViT for both
VQAv2 and GQA.
4 ComponentImportance Analysis
Understanding the relative importance of different components in vision-language models under
quantization presents unique analytical challenges. The relationship between bit precision and
performance is complex and potentially non-linear, with interactions between components that are
difficult to capture with simple models. Our preliminary experiments showed that linear approaches
yield poor fits (R2 < 0.20), indicating the need for more sophisticated methods.
4.1 Setup
Wepropose a framework based on three complementary tree-based importance analysis techniques
that can effectively capture non-linear relationships and interaction effects. These methods provide
different perspectives on component importance, allowing us to establish a consensus ranking that is
robust to the limitations of any single approach.
Random Forest Feature Importance requires Random Forest regressors trained to predict perfor
mance scores based on the bit precision of each component:
score = f(vit_bits,qformer_bits,llm_bits)
The Random Forest naturally captures non-linear relationships and interactions between features.
We extract the built-in feature importance metric, which measures the total reduction in impurity
(variance) attributable to each feature across all trees. To quantify uncertainty, we implement bootstrap
resampling (n = 100) to obtain 95% confidence intervals.
6
12 14 16
0
10
20
30
40
50
AWQ, VQAv2 Accuracy
6 8 10 12 14 16
0
10
20
30
40
50
4 6 8 10 12 14 16
0
10
20
30
40
50
12 14 16
Bits per Weight
0
10
20
30
40
50
GPTQ, VQAv2 Accuracy
Quantization Bits
2-bit ViT
16-bit ViT
2-bit Q-Former
16-bit Q-Former
6 8 10 12 14 16
Bits per Weight
0
10
20
30
40
50
Quantization Bits
2-bit Q-Former
16-bit Q-Former
2-bit LLM
16-bit LLM
4 6 8 10 12 14 16
Bits per Weight
0
10
20
30
40
50
Quantization Bits
2-bit ViT
16-bit ViT
2-bit LLM
16-bit LLM
Figure6: PairwisequantizationofBLIP-2’sViT,Q-Former, andLLMforVQAv2.Wedraw
attentiontohowcomponentsinteractwitheachotherwhenquantized.Notably,thequantizationof
multiplecomponentsyieldsworseperformancethansinglecomponentquantization.
PermutationFeatureImportanceprovidesamodel-agnosticmeasureof featurerelevanceby
quantifyinghowmuchmodelperformancedegradeswhenafeature’svaluesarerandomlyshuffled:
Ij=EX,y[L(f,Xj,y)]−EX,y[L(f,X,y)]
whereListheperformanceloss,Xj isthedatasetwithfeaturejpermuted,andIj istheimportance
offeaturej.Bybreakingtherelationshipbetweenthefeatureandthetarget,wecanmeasurehow
muchthemodelreliesonthatfeatureforpredictionaccuracy.Weperform50permutationiterations
andcalculateconfidenceintervalstoensurereliableestimates.
SHapleyAdditiveexPlanations(SHAP)[59]provideagame-theoreticapproachtofeatureattri
butionbasedonShapleyvalues. Thismethodcalculatesthecontributionofeachfeaturetoeach
predictionbyconsideringallpossiblecombinationsoffeatures:
ϕj=
S⊆N\{j}
|S|!(|N|−|S|−1)!
|N|! [fx(S∪{j})−fx(S)]
SHAPvalueshaveseveraldesirableproperties: theysumtothedifferencebetweentheactual
predictionandtheaverageprediction, theyareconsistent(amodel’sdependenceonafeaturecan
onlyincreasewhenthefeature’simportanceincreases),andtheyaccountforfeatureinteractions.We
useTreeExplainer[60],whichefficientlycomputesSHAPvaluesfortree-basedmodels.Themean
absoluteSHAPvalueforeachcomponentprovidesameasureofglobalimportancethataccountsfor
bothpositiveandnegativeeffectsacrossthedataset.
ConsensusRanking
Toestablisharobust,method-agnosticimportanceranking,weimplementaconsensusapproach
thatnormalizesandaggregatesresultsacrossall threefeatureattributiontechniques. Foreach
method(RandomForest,Permutation,SHAP),wefirstnormalizecomponent importancescores
tosumto100%, ensuringcomparablescales.Wethenaveragethesenormalizedvaluesacross
7
Figure 7: BLIP-2 Quantization Component Performance on COCO and VQAv2 tasks. We report
normalized importance percentages of the Vision Transformer (ViT), Q-Former and LLM components
under GPTQ and AWQ.Component importance varies across quantization technique and task. GPTQ
has a more balanced distribution of importances while AWQ has a stronger skew towards the LLM.
Figure 8: LLaVA Quantization Component Performance on GQA and VQAv2 tasks. We report
normalized importance percentages of the model components under GPTQ and AWQ.
methods to compute a consensus importance percentage for each component (ViT, Q-Former, LLM).
This approach mitigates method-specific biases while capturing complementary aspects of feature
importance—predictive power from Random Forest, direct performance impact from Permutation
analysis, and instance-level contributions from SHAP. The resulting consensus percentages, presented
in the appendix, directly inform optimal bit-width allocation strategies across model components.
4.2 Results
Quantization method dramatically shifts component importance. The quantization method
significantly affects component importance, with AWQ consistently assigning greater importance to
the LLM compared to GPTQ, which distributes effects more evenly across components. In BLIP-2,
for COCO captioning, GPTQ balances importance at 50.4% for the ViT and 47.6% for the LLM,
while AWQ shifts this to 17.1% for the ViT and 80.5% for the LLM, as shown in Figure 7. This
difference persists across tasks: GPTQ results in 22-78% importance for the ViT (depending on
dataset), while AWQ results in ≥ 73% to the LLM in all cases that utilize it.
This trend extends to LLaVA as well, where AWQ results in the LLM having a much greater
importance after quantization. As shown in Figure 8, AWQ heavily favors the LLM with a 94.62% and
93.89% importance to the LLM for both GQA and VQAv2, respectively, compared to GPTQ’s more
balanced distribution of 79.13% and 72.15% LLM importance for GQA and VQAv2, respectively.
Despite the ViT accounting for only 4.3% of the total parameters, it maintains 20-30% importance
under GPTQ for both tasks, proving that importance is not directly proportional to component size.
These shifts suggest AWQ’s activation-aware approach prioritizes the LLM’s large weight matrices,
concentrating quantization effects there, while GPTQ’s Hessian-based method captures broader
8
component interactions. Thus, bit-allocation strategies must adapt to the quantization method, as
AWQ’s LLMfocus contrasts with GPTQ’s balanced distribution.
Task characteristics drive component importance variations. Task demands strongly dictate
component importance, with consensus values varying across datasets. For BLIP-2, the COCO
captioning task results in a balanced importance distribution under GPTQ (50.4% ViT, 47.6% LLM),
suggesting equivalent reliance on both visual encoding and text generation. Retrieval tasks (Flickr),
which utilize only the ViT and Q-Former without an LLM component, show ViT dominance under
both methods (ViT ≥ 70%). Notably, the Q-Former’s importance increases substantially in these
retrieval tasks (15-30% versus < 3% in other tasks), highlighting its critical role in aligning visual
and textual embeddings when no decoder is present to compensate for misalignment.
Conversely, reasoning-intensive VQA tasks shift importance dramatically toward the LLM. For
BLIP-2, the LLM accounts for 73.1-76.7% importance with GPTQ and 98.3-98.6% with AWQ on
VQAv2 and GQA tasks. In LLaVA, we observe a similar pattern: for VQAv2, GPTQ allocates
72.15% importance to the LLM and 27.85% to the vision model, while for GQA, the distribution is
79.13% LLM and 20.87% vision model. Under AWQ, this skew becomes even more pronounced,
with the LLM accounting for a 94.62% and 93.89% importance and the vision model only 5.38% and
6.11% for GQAandVQAv2, respectively. These patterns correspond to underlying task requirements:
the LLM is primarily responsible for the sophisticated language generation these tasks require.
Architectural layout and component interplay shape quantization patterns.
The architectural layout and interplay of components strongly shape quantization patterns, redis
tributing importance based on their roles and sequential dependencies. For BLIP-2’s retrieval tasks
(Flickr–Text and Flickr–Image), which exclude the LLM, the Q-Former’s importance jumps to
21.6–29.7% under GPTQ and 15.3–26.8% under AWQ, compared to < 3% in tasks with the LLM (
Figure 7). This shift may indicate the Q-Former’s greater alignment role when no LLM compensates
for visual-textual mismatches. In contrast, the Vision Model in LLaVA, connected directly to the LLM
via a smaller linear projection layer, shows heightened importance of 20.87–27.85% under GPTQ for
GQAandVQAv2,respectively (Figure 8), possibly because the limited capacity of the projection
layer increases the Vision Model’s role in visual processing, despite its mere 4.3% parameter share.
Pairwise experiments further highlight this interplay: in BLIP-2, quantizing both ViT and LLM
simultaneously worsens performance more than individual quantization (Figure 6), revealing non
additive effects from sequential dependencies. The Q-Former’s end-position in retrieval tasks
increases its sensitivity, while LLaVA’s Vision-LLM linkage amplifies the Vision Model’s impact.
Thus, bit-allocation strategies must account for architectural context and component dependencies,
beyond method-specific or task-driven shifts, to optimize quantization across diverse VLMs.
5 Conclusion
In this work, we systematically investigated the effects of quantization on vision-language models,
with a focus on understanding how different components of multimodal architectures like BLIP-2 and
LLaVA respond to reduced precision. Our experiments with uniform quantization and state-of-the-art
methods such as GPTQ and AWQ reveal that model components exhibit distinct sensitivities to quan
tization, often with the language model being the most critical, especially in tasks requiring complex
reasoning. We demonstrate that SOTA quantization techniques can maintain high performance at
significantly lower bit widths compared to uniform approaches, and that the choice of quantization
method can substantially alter the relative importance of different model components.
These insights provide practical guidance for optimizing the performance-efficiency tradeoff in
multimodal systems, enabling more practical deployment scenarios. While our study is limited to
simulated quantization without capturing end-to-end latency or hardware-specific optimizations,
these constraints point to natural extensions in future work. Our open-source contributions of
calibration implementations, comprehensive ablation studies, and component analysis tools enable
the community to build upon these findings. As the field expands beyond vision-language to
incorporate other modalities, the approach developed in this paper provides a systematic methodology
for quantifying component importance across heterogeneous architectures. This understanding
can directly inform practical tradeoffs in multimodal model compression for resource-constrained
environments, from mobile devices to edge computing platforms.
9
References
[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei, “Scaling laws for neural language models,” 2020.
[2] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. Van Baalen, and T. Blankevoort, “A white
paper on neural network quantization,” arXiv preprint arXiv:2106.08295, 2021.
[3] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian, “A
comprehensive overview of large language models,” 2024.
[4] N. Dhar, B. Deng, D. Lo, X. Wu, L. Zhao, and K. Suo, “An empirical analysis and resource footprint
study of deploying large language models on edge devices,” in Proceedings of the 2024 ACM Southeast
Conference, pp. 69–76, 2024.
[5] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-training with frozen
image encoders and large language models,” 2023.
[6] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” 2023.
[7] F. Shu, L. Zhang, H. Jiang, and C. Xie, “Audio-visual llm for video understanding,” arXiv preprint
arXiv:2312.06720, 2023.
[8] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “Gptq: Accurate post-training quantization for
generative pre-trained transformers,” 2023.
[9] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, “Awq:
Activation-aware weight quantization for llm compression and acceleration,” 2024.
[10] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, “Howto100m: Learning a
text-video embedding by watching hundred million narrated video clips,” 2019.
[11] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language
supervision,” 2021.
[12] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, and T. Duerig, “Scaling
up visual and vision-language representation learning with noisy text supervision,” 2021.
[13] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, “Frozen in time: A joint video and image encoder for
end-to-end retrieval,” 2022.
[14] J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language models for vision tasks: A survey,” 2023.
[15] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,
M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L.
Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Bi´nkowski, R. Barreira, O. Vinyals,
A. Zisserman, and K. Simonyan, “Flamingo: a visual language model for few-shot learning,” in Advances
in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and
A. Oh, eds.), vol. 35, pp. 23716–23736, Curran Associates, Inc., 2022.
[16] M. Tsimpoukelli, J. L. Menick, S. Cabi, S. M. A. Eslami, O. Vinyals, and F. Hill, “Multimodal few-shot
learning with frozen language models,” in Advances in Neural Information Processing Systems (M. Ranzato,
A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), vol. 34, pp. 200–212, Curran Associates,
Inc., 2021.
[17] R. Mokady, A. Hertz, and A. H. Bermano, “Clipcap: Clip prefix for image captioning,” 2021.
[18] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and
M. Elhoseiny, “Minigpt-v2: large language model as a unified interface for vision-language multi-task
learning,” arXiv preprint arXiv:2310.09478, 2023.
[19] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing vision-language understanding
with advanced large language models,” arXiv preprint arXiv:2304.10592, 2023.
[20] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre,
S. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt, “Openflamingo:
An open-source framework for training large autoregressive vision-language models,” arXiv preprint
arXiv:2308.01390, 2023.
10
[21] D. A. Hudson and C. D. Manning, “Gqa: A new dataset for real-world visual reasoning and compositional
question answering,” 2019.
[22] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in vqa matter: Elevating the
role of image understanding in visual question answering,” 2017.
[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min
derer, G. Heigold, S. Gelly, et al., “An image is worth 16x16 words: Transformers for image recognition at
scale,” arXiv preprint arXiv:2010.11929, 2020.
[24] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,
T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,
“Opt: Open pre-trained transformer language models,” 2022.
[25] Y. LeCun, J. Denker, and S. Solla, “Optimal brain damage,” in Advances in Neural Information Processing
Systems (D. Touretzky, ed.), vol. 2, Morgan-Kaufmann, 1989.
[26] X. Dong, S. Chen, and S. J. Pan, “Learning to prune deep neural networks via layer-wise optimal brain
surgeon,” 2017.
[27] S. Park, J. Lee, S. Mo, and J. Shin, “Lookahead: A far-sighted alternative of magnitude-based pruning,”
2020.
[28] X. XIAO, Z. Wang, and S. Rajasekaran, “Autoprune: Automatic network pruning by regularizing aux
iliary parameters,” in Advances in Neural Information Processing Systems (H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, eds.), vol. 32, Curran Associates, Inc., 2019.
[29] Y.He, J.Lin, Z.Liu, H.Wang, L.-J. Li, and S. Han, AMC:AutoMLforModelCompressionandAcceleration
on Mobile Devices, p. 815–832. Springer International Publishing, 2018.
[30] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han, M. Gao, C.-Y. Lin, and L. S. Davis, “Nisp:
Pruning networks using neuron importance score propagation,” 2018.
[31] J.-H. Luo, J. Wu, and W. Lin, “Thinet: A filter level pruning method for deep neural network compression,”
2017.
[32] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” 2015.
[33] S. Ahn, S. X. Hu, A. C. Damianou, N. D. Lawrence, and Z. Dai, “Variational information distillation for
knowledge transfer,” CoRR, vol. abs/1904.05835, 2019.
[34] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and L.-J. Li, “Learning from noisy labels with distillation,” 2017.
[35] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowledge distillation: Fast optimization, network
minimization and transfer learning,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), July 2017.
[36] H. Yin, P. Molchanov, Z. Li, J. M. Alvarez, A. Mallya, D. Hoiem, N. K. Jha, and J. Kautz, “Dreaming to
distill: Data-free knowledge transfer via deepinversion,” 2020.
[37] Y. Gu, L. Dong, F. Wei, and M. Huang, “Minillm: Knowledge distillation of large language models,” 2024.
[38] Y.Guo, “Asurveyonmethodsandtheories of quantized neural networks,” arXiv preprint arXiv:1808.04752,
2018.
[39] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, “Quantized neural networks: Training
neural networks with low precision weights and activations,” journal of machine learning research, vol. 18,
no. 187, pp. 1–30, 2018.
[40] M. Nagel, R. A. Amjad, M. van Baalen, C. Louizos, and T. Blankevoort, “Up or down? adaptive rounding
for post-training quantization,” 2020.
[41] I. Hubara, Y. Nahshan, Y. Hanani, R. Banner, and D. Soudry, “Improving post training neural quantization:
Layer-wise calibration and integer programming,” 2020.
[42] P. Wang, Q. Chen, X. He, and J. Cheng, “Towards accurate post-training network quantization via bit-split
and stitching,” in International Conference on Machine Learning, pp. 9847–9856, PMLR, 2020.
[43] Y. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu, “Brecq: Pushing the limit of
post-training quantization by block reconstruction,” 2021.
11
[44] E. Frantar, S. P. Singh, and D. Alistarh, “Optimal brain compression: A framework for accurate post
training quantization and pruning,” 2023.
[45] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar, S. Ashkboos, A. Borzunov, T. Hoe
f
ler, and D. Alistarh, “Spqr: A sparse-quantized representation for near-lossless llm weight compression,”
2023.
[46] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park, “Owq: Outlier-aware weight quantization for efficient
f
ine-tuning and inference of large language models,” 2024.
[47] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “Llm.int8(): 8-bit matrix multiplication for
transformers at scale,” 2022.
[48] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, “Quantization
and training of neural networks for efficient integer-arithmetic-only inference,” 2017.
[49] Z. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad, Y. Shi, R. Krishnamoorthi, and V. Chandra,
“Llm-qat: Data-free quantization aware training for large language models,” 2023.
[50] M. Nagel, M. Fournarakis, Y. Bondarenko, and T. Blankevoort, “Overcoming oscillations in quantization
aware training,” in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri,
S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine
Learning Research, pp. 16318–16330, PMLR, 17–23 Jul 2022.
[51] M.Chen, W.Shao, P. Xu, J. Wang, P. Gao, K. Zhang, and P. Luo, “Efficientqat: Efficient quantization-aware
training for large language models,” 2024.
[52] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang, P. Gao, Y. Qiao, and P. Luo, “Omniquant:
Omnidirectionally calibrated quantization for large language models,” 2024.
[53] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei, “Bitnet:
Scaling 1-bit transformers for large language models,” 2023.
[54] S. Ma, H. Wang, L. Ma, L. Wang, W. Wang, S. Huang, L. Dong, R. Wang, J. Xue, and F. Wei, “The era of
1-bit llms: All large language models are in 1.58 bits,” 2024.
[55] Y. Xu, X. Han, Z. Yang, S. Wang, Q. Zhu, Z. Liu, W. Liu, and W. Che, “Onebit: Towards extremely low-bit
large language models,” 2024.
[56] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, “A survey of quantization methods
for efficient neural network inference,” 2021.
[57] T. Chen, Z. Li, W. Xu, Z. Zhu, D. Li, L. Tian, E. Barsoum, P. Wang, and J. Cheng, “Ternaryllm: Ternarized
large language model,” 2024.
[58] D. Du, Y. Zhang, S. Cao, J. Guo, T. Cao, X. Chu, and N. Xu, “Bitdistiller: Unleashing the potential of
sub-4-bit llms via self-distillation,” 2024.
[59] S. Lundberg and S.-I. Lee, “A unified approach to interpreting model predictions,” 2017.
[60] S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M. Prutkin, B. Nair, R. Katz, J. Himmelfarb, N. Bansal,
and S.-I. Lee, “Explainable ai for trees: From local explanations to global understanding,” 2019.
12
A Appendix
A.1 UniformQuantizationDerivation
Wedefinek-bituniformquantizationindepthasfollows.Givenfull-precisionweights,x,wefirst
normalizethemto[0,1]by
s(x)= x−wmin
wmax−wmin
wherewmaxandwminrefertotheper-tensormaximumandminimumweightvaluesofx,respectively.
Thenormalizedweightsarethenassignedtotheirclosestk-bitintegervalues,yieldingdiscretized
weightsˆxwith
ˆx= 1
2k−1 ·round((2k−1)∗s(x))
Finally, thediscretizedweights,ˆx,arereturnedtotheiroriginalscale,yieldingquantizedweights
Q(x)with
Q(x)=(wmin−wmax)·ˆx+wmin
A.2 OptimizingPerformanceSupplemental
5.0 7.5 10.0 12.5 15.0
Bits per Weight
0.0
0.2
0.4
0.6
0.8
1.0
1.2
BLIP-2, COCO, CIDEr
Quantized Model
ViT
V+Q
 
Q-Former
V+L
All
LLM
Q+L
5.0 7.5 10.0 12.5 15.0
Bits per Weight
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Quantized Layer
Attn FF Both
5.0 7.5 10.0 12.5 15.0
Bits per Weight
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Quantized Group
Front
F+M
 
Middle
F+E
All
End
M+E
Figure9:UniformquantizationimpactonCOCOcaptioningforBLIP-2.Weprovidefullresultsto
complementthezoomed-inresultsinFigure3.Blackstarsshowresultswhenwequantizetheentire
pipelineat4,5,6,8,and16bits.Thebestperformance-sizetradeoffstendtocomewhenwequantize
theentirepipeline.
WeprovideFigure9tocomplement theresultsinFigure3, thistimebyshowingthefull range
ofbitwidthsandperformancesthanwetest, ratherthanonlyfocusingontheportionthatyields
highperformance.Wealsogiveresultsonanothertask,retrievalinFigure10.Ourfindingsremain
consistentevenforthisothertask.
WealsocomplementtheresultsinFigure6withFigure11.Thisshowsthatourfindingsholdonthe
GQAdataset.
A.3 ComponentImportanceSupplemental
WegivecomponentimportanceanalysisacrossmultipletasksanddatasetsforBLIP-2inFigure12.
OurfindingsremainconsistentwiththoseinFigure7evenconsideringtheseadditionaldatasets.We
13
2.5 5.0 7.5 10.0 12.5 15.0
Bits per Weight
0
20
40
60
80
100
BLIP-2, Flickr, Recall @ 1
Quantized Model
ViT Q-Former Both
2.5 5.0 7.5 10.0 12.5 15.0
Bits per Weight
0
20
40
60
80
100
Quantized Layer
Attn FF Both
2.5 5.0 7.5 10.0 12.5 15.0
Bits per Weight
0
20
40
60
80
100
Quantized Group
Front
F+M
 
Middle
F+E
All
End
M+E
Figure10:UniformquantizationimpactonFLICKRtext-to-imageretrievalforBLIP-2.Wedraw
attentionfirsttowhichmodelcomponentwequantize(left), thenlayertype(middle), thengroup
(right).Thebestperformance-sizetradeoffstendtocomewhenwequantizetheentirepipeline.
12 14 16
0
10
20
30
AWQ, GQA Accuracy
6 8 10 12 14 16
0
10
20
30
4 6 8 10 12 14 16
0
10
20
30
12 14 16
Bits per Weight
0
10
20
30
GPTQ, GQA Accuracy
Quantization Bits
2-bit ViT
16-bit ViT
2-bit Q-Former
16-bit Q-Former
6 8 10 12 14 16
Bits per Weight
0
10
20
30
Quantization Bits
2-bit Q-Former
16-bit Q-Former
2-bit LLM
16-bit LLM
4 6 8 10 12 14 16
Bits per Weight
0
10
20
30
Quantization Bits
2-bit ViT
16-bit ViT
2-bit LLM
16-bit LLM
Figure11:AWQandGPTQpairwisequantizationofBLIP-2’svisionencoder,Q-Former,and
LLMforGQA.
showthisfiguremainlyforthoroughness,alongwithTable1,whichprovidesthenumbersreflected
intheseplots.
14
Figure12:Componentimportanceanalysisacrossquantizationmethodsanddatasets.This
figureshowstherelativeimportanceofdifferentBLIP-2components(LLM,VisionTransformer,
andQFormer)whenquantizedusingGPTQ(toprow)andAWQ(bottomrow)acrossfiveevaluation
datasets.Thethreecolorsrepresentdifferentfeatureimportanceanalysismethods:RandomForest,
Permutation,SHAP,andtheConsensusbetweenthethree.
Table1:Consensusfeature-importance(%)ofmodelcomponents.
Model Method Dataset ViT QFormer LLM
BLIP-2
GPTQ
COCO 50.4 2.0 47.6
GQA 22.6 0.7 76.7
VQAv2 25.3 1.5 73.1
Flickr–Text 70.3 29.7
Flickr–Image 78.4 21.6
AWQ
COCO 17.1 2.5 80.5
GQA 1.0 0.4 98.6
VQAv2 1.5 0.2 98.3
Flickr–Text 73.2 26.8
Flickr–Image 84.7 15.3
LLaVA
GPTQ GQA 20.87– 79.13
VQAv2 27.85– 72.15
AWQ GQA 5.38– 94.62
VQAv2 6.11– 93.89
15